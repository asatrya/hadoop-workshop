version: "3"
services:
  namenode:
    image: trivadis/apache-hadoop-namenode:2.0.0-hadoop3.1.1-java8
    container_name: namenode
    hostname: namenode
    volumes:
      - ./container-volume/namenode:/hadoop/dfs/name
      - ./data-transfer:/data-transfer
    ports:
      - "9870:9870"
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./conf/hadoop.env
    restart: always

  datanode-1:
    image: trivadis/apache-hadoop-datanode:2.0.0-hadoop3.1.1-java8
    container_name: datanode-1
    volumes:
      - ./container-volume/datanode-1:/hadoop/dfs/data
    ports:
      - "9864:9864"
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./conf/hadoop.env
    restart: always

  datanode-2:
    image: trivadis/apache-hadoop-datanode:2.0.0-hadoop3.1.1-java8
    container_name: datanode-2
    volumes:
      - ./container-volume/datanode-2:/hadoop/dfs/data
    ports:
      - "9865:9864"
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./conf/hadoop.env
    restart: always

  hue:
    image: gethue/hue:4.4.0
    container_name: hue
    hostname: hue
    dns: 8.8.8.8
    ports:
      - "28888:8888"
    volumes:
      - ./conf/hue.ini:/usr/share/hue/desktop/conf/hue.ini
    depends_on:
      - hue-postgres
    restart: always

  hue-postgres:
    image: postgres:10
    container_name: hue-postgres
    hostname: hue-postgres
    environment:
      POSTGRES_DB: hue
      POSTGRES_PASSWORD: hue
      POSTGRES_USER: hue
    restart: always

  minio:
    image: minio/minio
    container_name: minio
    hostname: minio
    ports:
      - "9000:9000"
    volumes:
      - "./container-volume/minio/data/:/data"
    #      - './minio/config:/root/.minio'
    environment:
      MINIO_ACCESS_KEY: V42FCGRVMK24JJ8DHUYG
      MINIO_SECRET_KEY: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
    command: server /data
    restart: always

  awscli:
    image: xueshanf/awscli
    container_name: awscli
    hostname: awscli
    volumes:
      - './conf/s3cfg:/root/.s3cfg'
      - './data-transfer:/data-transfer'
#      - './minio/config:/root/.minio'
    environment:
      AWS_ACCESS_KEY_ID: V42FCGRVMK24JJ8DHUYG
      AWS_SECRET_ACCESS_KEY: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
    command: tail -f /dev/null
    restart: always

  spark-master:
    image: trivadis/apache-spark-master:2.4.4-hadoop2.7 
    container_name: spark-master
    hostname: spark-master
    ports:
      - 6066:6066
      - 7077:7077
      - 8080:8080
    env_file:
      - ./conf/hadoop.env  
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
      INIT_DAEMON_STEP: setup_spark
#      CORE_CONF_fs_defaultFS: hdfs://namenode:8020
    volumes:
      - ./conf/spark/spark-defaults.conf:/spark/conf/spark-defaults.conf
      - ./credentials/s3.jceks:/credentials/s3.jceks
    restart: always

  spark-worker-1:
    image: trivadis/apache-spark-worker:2.4.4-hadoop2.7
    container_name: spark-worker-1
    hostname: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    env_file:
      - ./conf/hadoop.env  
    environment:
      SPARK_MASTER: "spark://spark-master:7077"
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
#      SPARK_WORKER_CORES: 2
#      SPARK_WORKER_MEMORY: 1g
      SPARK_WORKER_WEBUI_PORT: "8081"
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
    volumes:
      - ./conf/spark/spark-defaults.conf:/spark/conf/spark-defaults.conf
    restart: always

  spark-worker-2:
    image: trivadis/apache-spark-worker:2.4.4-hadoop2.7
    container_name: spark-worker-2
    hostname: spark-worker-2
    depends_on:
      - spark-master
    ports:
      - "8082:8082"
    env_file:
      - ./conf/hadoop.env  
    environment:
      SPARK_MASTER: "spark://spark-master:7077"
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
#      SPARK_WORKER_CORES: 2
#      SPARK_WORKER_MEMORY: 1g
      SPARK_WORKER_WEBUI_PORT: "8082"
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
    volumes:
      - ./conf/spark/spark-defaults.conf:/spark/conf/spark-defaults.conf
    restart: always

  zeppelin:
    image: trivadis/apache-zeppelin:0.8.2-hadoop2.7-spark2.4.4
    container_name: zeppelin
    hostname: zeppelin
    ports:
      - "38081:8080"
#      - "4040:4040"
#      - "42331:42331"    
    env_file:
      - ./conf/hadoop.env
    environment:
      # AWS Credentials
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}

      ZEPPELIN_ADDR: "0.0.0.0"
      ZEPPELIN_PORT: "8080"
      ZEPPELIN_INTERPRETER_CONNECT_TIMEOUT: 120000
      SPARK_MASTER: "spark://spark-master:7077"

      # set spark-master for Zeppelin interpreter
      MASTER: "spark://spark-master:7077"
      SPARK_DRIVER_HOST: zeppelin
      SPARK_DRIVER_BINDADDRESS: "0.0.0.0"
      PYSPARK_PYTHON: "python3"
# no longer necessary with 0.8.2 of Zepplin      
#      - SPARK_SUBMIT_OPTIONS="--packages org.apache.commons:commons-lang3:3.5"
      # enableV4 to make it work with AWS Frankfurt region
      SPARK_SUBMIT_OPTIONS: "--conf spark.driver.extraJavaOptions=-Dcom.amazonaws.services.s3.enableV4 --conf spark.executor.extraJavaOptions=-Dcom.amazonaws.services.s3.enableV4"
    volumes:
      - ./conf/spark/spark-defaults.conf:/spark/conf/spark-defaults.conf
      - ./conf/zeppelin/shiro.ini:/opt/zeppelin/conf/shiro.ini
      - ./conf/zeppelin/interpreter-setting.json:/opt/zeppelin/interpreter/spark/interpreter-setting.json
      - ./data-transfer:/data-transfer
    restart: always
